sample_rate: 22050
n_fft: 1024
hop_length: 256
n_mels: 80
vq_vae_embed_dim: 128

model:
  vq_vae:
    _target_: mel_to_wav_modules.VQVAEModule
    n_mels: ${n_mels}
    channel_size: 128
    resblock_kernel_sizes: [1, 3, 5]
    n_resblocks: 3
    embed_dim: ${vq_vae_embed_dim}
    n_embed: 256
  l1_factor: 45
  latent_loss_weight: 10
  generator:
    _target_: mel_to_wav_modules.Generator
    resblock: 1
    upsample_rates: [ 8,8,2,2 ]
    upsample_kernel_sizes: [ 16,16,4,4 ]
    upsample_initial_channel: 512
    resblock_kernel_sizes: [ 3,7,11 ]
    resblock_dilation_sizes: [ [ 1,3,5 ], [ 1,3,5 ], [ 1,3,5 ] ]
    initial_input_size: ${vq_vae_embed_dim}
  mel_cfg:
    # _target_: utils.ComputeMel
    sample_rate: ${sample_rate}
    n_fft: ${n_fft}
    hop_length: ${hop_length}
    n_mels: ${n_mels}
    f_max: 8000

  vocoder_sample_rate: 22050
  vocoder_hop: ${hop_length}


optim:
  _target_: torch.optim.AdamW
  lr: 0.00002
  betas: [ 0.8, 0.99 ]

train_cfg:
  dataset:
    _target_: dataset.ParallelAlignedDataset
    # manifest: '/root/storage/TTS/avc/manifests/avc_train.txt'
    manifest: '/root/storage/TTS/avc/manifests/avc_train_gen_w_predicted_durs.txt'
    # target_speaker_path: '/root/storage/TTS/LJSpeech-1.1/wavs'
    target_speaker_path: '/root/storage/TTS/avc/generated_ljs'
    n_frames: 32 # 128 # 64
    mel_cfg:
      # _target_: utils.ComputeMel
      sample_rate: ${sample_rate}
      n_fft: ${n_fft}
      hop_length: ${hop_length}
      n_mels: ${n_mels}
      f_max: 8000

  dataloader:
    batch_size: 8 # 64 # 32 total should be 128
    num_workers: 4
    shuffle: True


val_cfg:
  dataset:
    _target_: dataset.ParallelAlignedDataset
    manifest: '/root/storage/TTS/avc/manifests/val_manifest_right_order.txt'
    # target_speaker_path: '/root/storage/TTS/LJSpeech-1.1/wavs'
    target_speaker_path: '/root/storage/TTS/avc/generated_ljs'
    n_frames: -1
    mel_cfg:
      # _target_: utils.ComputeMel
      sample_rate: ${sample_rate}
      n_fft: ${n_fft}
      hop_length: ${hop_length}
      n_mels: ${n_mels}
      f_max: 8000

  dataloader:
    batch_size: 16
    num_workers: 4
    shuffle: False

trainer:
  gpus: -1 # number of gpus
  max_steps: 500000
  num_nodes: 1
  accelerator: ddp
  accumulate_grad_batches: 1
  logger: True
  flush_logs_every_n_steps: 200
  log_every_n_steps: 100
  val_check_interval: 1.0